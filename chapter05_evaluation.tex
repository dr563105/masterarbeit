\chapter{Evaluation}
In this chapter, the workflow explained in last chapter is evaluated and results are
presented.

Before showing the evaluation, it is necessary to define training and testing conditions
that can be easily used by others to verify the results.
\begin{figure}[h]
    \centering
    \def\svgwidth{0.3\textwidth}
    \input{figures/inkscape/datasets_general.pdf_tex}
    \caption{Datasets distribution}
    \label{fig:datasetsdistribution}
\end{figure}

We have three datasets that can be used for training and evaluation.
\begin{enumerate}
    \item Dataset 1 - Contains 100,000 raw data as seen in fig. \ref{fig:datasetsdistribution}. It is collected in no traffic
        environment, doing straight driving without any sudden turning. The data is using
        San Francisco map and driven during afternoon. This dataset has only data
        representing centre camera pointed ahead, parallel to the ground and right camera
        pointed to the ground at an angle $20^{\circ}$. The control commands include
        acceleration, throttle, braking, and steering angle values. \label{chapter05list:ds1}
    \item Dataset 2 - Also contains 100,000 raw data. It is, however, collected with traffic
        where the cars stop at signal intersections for a longer time than dataset 3. This
        dataset is also collect in San Francisco map and during afternoon. It contains a
        centre camera, right camera like dataset 1, left camera similar to right camera by
        pointing at an angle $20^{\circ}$ to the ground, depth camera sensors placed at
        centre, left and right just like RGB cameras. The control commands are same as
        dataset 1.
    \item Dataset 3 - Contains 270,000 raw data. It is collected while driving around San
        Francisco. About 200,000 data is collected while driving in the afternoon. About
        20,000 in different weather and light conditions. About 50,000 entries are
        collected in a different circular circuit map called CubeTown. In addition to RGB
        and depth cameras distributed just as dataset 2, a segmentation camera is kept
        next to centre RGB camera facing forward, and a radar sensor just in front of the
        car near the hood also facing forward.

\end{enumerate}

\section*{Evaluation setup}
\begin{table}[t]
    \centering
    \begin{tabular}{lll}
    \toprule
    Hyperparameter & Value & Explanation \\\midrule
      $\alpha$& 0.0001& Learning rate of the adam optimizer \\
      Epochs & 50 & Number of cycles the model is trained for \\
      $\beta$ & 128 & Batch size \\
      Shuffle & True & Shuffling of the data while training \\
        CNN\_FM & [24, 36, 48, 64, 64] & Values of feature maps channel of each
        convolutional layer \\
        CNN\_Kernel\_size &(5,5) and (3,3) & Kernel or filter size for convolution \\
        CNN\_Stride & (2,2) and (1,1) & Stride parameter defining the step of the kernel
        \\
        Padding & same & Keeps output image's dimensions same as input \\
        Episode & 30s & Duration of an episode in seconds
        \\\bottomrule
\end{tabular}
\caption{Default hyperparameter setting}
\label{table:hyperparametersetting}
\end{table}
The table \ref{table:hyperparametersetting} gives an overview of all the default
hyperparameter setting. Whenever something related to this setting is changed, the changes
are mentioned.

While evaluating, a testing parameter \textit{episode} is used. Each episode lasts 30 seconds. A timer is started for 30 seconds and
the  model is tested for collisions. If a collision happens, the time at which collision
happened is noted.

As supervised learning is used, the models have to be tested/validated with unknown data
to determine its capability. Hence the datasets are split 80-20. Meaning 80\% is train
data and 20\% validation data. The optimizer \textit{Adam} takes the 20\% data to test the
trained model. Training data leads to training loss and test data to validation loss.

Also till a single dataset is chosen, all datasets are of equal data entries.

\section{Determine which datasets and best lighting conditions to test the model}
\label{chapter05subsec:setup1}
All three datasets are used. The test is conducted in San Francisco map without traffic
option switched ON. By varying the light conditions to morning, afternoon and evening, we
observe how light influences the prediction of output(see table \ref{table:timeoftheday}).

The steering angle is a continuous value ranging between -1 and 1(negative values to
turn left and positive values to turn right), a continuous loss function has to be used.
Because of that \textit{mean square error}(MSE) as loss function is chosen.
Only steering angle is predicted
and a steady velocity of 3 meter per second is used. An episode length of 30s is used.
When a collision is observed, the time of collision and the number of collisions are noted down.
\begin{table}[!ht]
    \centering
\begin{tabular}{cccc}
    \toprule
    time(in 24 hrs standard) & Morning & Afternoon & Evening \\\midrule
      & 7:30 & 15:30 & 18:30 \\\bottomrule
\end{tabular}
\caption{Time of the day}
\label{table:timeoftheday}
\end{table}

\begin{figure}[h]
	\centering
    \def\svgwidth{0.9\textwidth}
    \input{figures/inkscape/lightvscollisionvstraffic.pdf_tex} %use full path to know the location of pdftex
    \caption{a) Datasets vs Light Conditions vs Collisions.
        b) Afternoon - Datasets vs Traffic vs Number of Collisions.
    c) Average number of collisions in percentage}
    \label{fig:dsvslcvstrafficAll}
\end{figure}

It is seen from fig. \ref{fig:dsvslcvstrafficAll}(a) that afternoon time provides the best light conditions for all the three
datasets. Dataset 1 and 3 perform equally across the three lighting conditions.

If the percentage of number of collisions with traffic toggled ON, as shown in fig. \ref{fig:dsvslcvstrafficAll}(c), is calculated, dataset 3 performs the best among
the datasets for morning and afternoon part of the day.

\subsection{Datasets performance during afternoon if traffic is enabled}
All three datasets are again used. The time is fixed at 15:30. The traffic is toggled ON.
From fig. \ref{fig:dsvslcvstrafficAll}(b), we can observe that all three datasets do
well even in traffic. However, it is surprising to see dataset 1 which had no traffic
while the dataset was collected, performs remarkably well when driven in traffic.
\subsection{Observations}
\begin{enumerate}
    \item All 3 datasets do well at afternoon time of the day.
    \item Predicting only steering angle with MSE as loss function works as seen from
        \ref{fig:dsvslcvstrafficAll}.
    \item Dataset 2 shows higher number of collisions. So it is better to avoid for
        further analysis.
\end{enumerate}

\section{Acceleration - Determine which activation and loss functions to use}

\subsection{Tanh as activation and MSE as loss functions}
Since acceleration and steering values in LGSVL range from \textit{-1} to \textit{1},
\textit{tanh} activation function is selected as the output dense layer activation
function.

A set of criteria are listed and the trained model is evaluated based on these conditions.
From table \ref{table:tanhmse} it can be observed that dataset 1 outperforms dataset 3 in
most of the conditions. Dataset 1 retains good steering control at high speeds and
turning, but acceleration skews steering angle when traffic is switched ON. Dataset 3 when
evaluated stops completely after moving a few metres. This causes difficulty in evaluating
according to the criteria. Hence it is assumed that this dataset fails to meet the
criteria.

One of the reasons as to why dataset 3 fails could be because the losses are much higher
than dataset 1 and the validation loss starts to overfit too quickly in the training(see
fig \ref{fig:ds1andd3tanhactivatonMSE}).
\begin{table}[h]
    \centering
\begin{tabular}{lcc}
    \toprule
    Criteria(Tanh/MSE). Rating 1 to 5. 1 being the lowest. & Dataset 1 & Dataset 3 \\\midrule
    Lane keeping/Drive straight  & 5 & 1  \\
    Gradual acceleration increase & 4 & 1\\
    Smooth braking behaviour observed & 4 & 1 \\
    Smooth steering control at high speed(10m/s) & 4 & 1 \\
    Smooth steering control at turnings & 3 & 1\\
    Detects traffic as dynamic objects & 5 & 1\\
    Navigates traffic smoothly & 2 & 1\\
    Stops at random places & 5 & 5 \\\bottomrule
\end{tabular}
\caption{Tanh/MSE - How the model evaluates to different criteria}
\label{table:tanhmse}
\end{table}
\begin{figure}[h]
	\centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/regressionModelsTanhActivation1.pdf_tex} %use full path to know the location of pdftex
    \caption{Datasets 1 vs 3 - Acceleration and Steering using Tanh activation and MSE loss
    functions.}
    \label{fig:ds1andd3tanhactivatonMSE}
\end{figure}

\subsection{Sigmoid as activation and MSE as loss functions}
The acceleration values are split into positive and negative values. Instead of negative
values an another variable we will call as \textit{braking} is introduced. Negative
acceleration values mean braking is active. Using this knowledge, sigmoid as activation function and mean
square error as loss function, a training is conducted for both datasets 1 and 3.

Criteria similar to table \ref{table:tanhmse} are put to test with this activation and
loss function. As seen in table \ref{table:sigmse}, both datasets fail to meet the
conditions. During evaluation, both datasets trained models, accelerate to a huge velocity
such as 40m/s and the steering cannot keep up, resulting in collisions.

Looking into their losses (see fig. \ref{fig:ds1andd3SigactivatonMSE}), don't really
explain much. Though this needs more investigation, for now we assume this activation or
loss function is not suitable.
\ref{fig:ds1andd3tanhactivatonMSE}.
\begin{table}[h]
    \centering
\begin{tabular}{lcc}
    \toprule
    Criteria(Sigmoid/MSE). Rating 1 to 5. 1 being the lowest & Dataset 1 & Dataset 3 \\\midrule
    Lane keeping/Drive straight  & 1 & 1  \\
    Gradual acceleration increase & 1 & 1\\
    Smooth braking behaviour observed & 1 & 1 \\
    Smooth steering control at high speed(10m/s) & 1 & 1 \\
    Smooth steering control at turnings & 1 & 1\\
    Detects traffic as dynamic objects & 1 & 1\\
    Navigates traffic smoothly & 1 & 1\\
    Stops at random places & 5 & 5 \\\bottomrule
\end{tabular}
\caption{Sigmoid/MSE - How the model evaluates to different criteria}
\label{table:sigmse}
\end{table}

\begin{figure}[h]
	\centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/regressionModelsSigActivation1.pdf_tex}
    \caption{Datasets 1 vs 3 - Acceleration and Steering using Sigmoid activation and MSE loss
    functions.}
    \label{fig:ds1andd3SigactivatonMSE}
\end{figure}

\subsection{Softmax as activation and Binary crossentropy as loss functions}
Both tanh and sigmoid activation functions couldn't give stable results to continue
pursuing with those parameters. Hence it is necessary to consider other functions which
may suit our needs.

Since acceleration are basically two discrete values, it would be worthy to try the
training as classification task. The goal of a classification task model is to classify to
which category the prediction belongs to. In our case, acceleration or braking. Hence
\textit{softmax} activation function is needed. As loss function \textit{binary
crossentropy} is used to classify as binary classes. Of course for steering angle,
being continuous, MSE is preferred.

From table \ref{table:softmaxandbce}, both datasets don't do well. Dataset 1 doesn't start
at all as braking class dominates the evaluation whereas dataset 3 starts to move the car
but resorts to brake indefinitely after a few metres of driving. This behaviour
necessitates further analysis into the datasets.
\begin{figure}[!ht]
	\centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/classificationbcCombo.pdf_tex}
    \caption{Dataset 1 vs 3 Validation loss - Binary crossentropy and Softmax functions}
    \label{fig:ds1andd3SoftactivatonBCE}
\end{figure}

\begin{table}[h]
    \centering
\begin{tabular}{lcc}
    \toprule
    Criteria(Softmax/Binary crossentropy). Rating 1 to 5. 1 being the lowest  & Dataset 1 & Dataset 3 \\\midrule
    Lane keeping/Drive straight  & 1 & 4  \\
    Gradual acceleration increase & 1 & 1\\
    Smooth braking behaviour observed & 1 & 1 \\
    Smooth steering control at high speed(10m/s) & 1 & 1 \\
    Smooth steering control at turnings & 1 & 1\\
    Detects traffic as dynamic objects & 1 & 1\\
    Navigates traffic smoothly & 1 & 1\\
    Stops at random places & 1 & 1 \\\bottomrule
\end{tabular}
\caption{Softmax/Binary crossentropy - How the model evaluates to different criteria}
\label{table:softmaxandbce}
\end{table}
\subsection*{Control commands distribution}
When the datasets are analysed for patterns of different states -- acceleration and
braking, distribution chart(fig \ref{fig:datasetscomparectrlcmds}) reveals that in addition to
acceleration and braking states, there is a third state called \textit{no action} where
the vehicle does absolutely nothing. In fact this state dominates in both datasets.
Because of this, the binary crossentropy obviously fails to meet the conditions.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/datasets_control_cmds.pdf_tex}
    \caption{Datasets 1 vs 3 control commands distribution}
    \label{fig:datasetscomparectrlcmds}
\end{figure}

\subsection{Softmax as activation and Categorical crossentropy loss functions}
So now we know that this dominant state \textit{no action} needs a separate label if
classification task has to be continued. After creating the label, we would then have
three labels -- acceleration, braking and no action. Hence, we use a new classification
loss function called \textit{categorical crossentropy}. This loss function classifies
model into each category.

\begin{table}[h]
    \centering
\begin{tabular}{lcc}
    \toprule
    Criteria(Softmax/Categorical crossentropy). Rating 1 to 5. 1 being the lowest & Dataset 1 & Dataset 3 \\\midrule
    Lane keeping/Drive straight  & 1 & 3  \\
    Gradual acceleration increase & 3 & 3\\
    Smooth braking behaviour observed & 1 & 3 \\
    Smooth steering control at high speed(10m/s) & 1 & 1 \\
    Smooth steering control at turnings & 1 & 1\\
    Avoids colliding into static objects & 1 & 1 \\
    Detects traffic as dynamic objects & 5 & 5\\
    Navigates traffic smoothly & 1 & 1\\
    Stops at random places & 5 & 5 \\\bottomrule
\end{tabular}
\caption{Softmax/Categorical crossentropy - How the model evaluates to different criteria}
\label{table:softmaxandcce}
\end{table}

From table \ref{table:softmaxandcce}, dataset 1 fails in most conditions and dataset 3
though performs well in 4 out of 9 conditions, shows bad steering behaviour at traffic or high
speeds. If the acceleration is controlled manually, the model reacts better and adapts
itself.

Since no action state dominates, it is necessary to continue as a classification task.
Also dataset 1 has only a small portion for acceleration and even smaller for
braking. Accordingly, dataset 3 is collected with an attempt to increase acceleration
and braking values share. However, since the vehicle most times has to drive straight, \textit{no action} state even dominates in dataset 3.

The important condition in a classification task is to have balanced classes.
Unfortunately in our case, this balance is not achieved. With this limitation, the
evaluation is carried forward.

\subsection{Observations}
\begin{enumerate}
    \item Steering angle uses tanh activation and MSE loss functions
    \item Classify acceleration prediction as classification task which means softmax as
        activation.
    \item Since acceleration carries three states, categorical crossentropy as loss
        function.
    \item Dataset 3 has more of acceleration and braking states than dataset 1. So dataset
        3 is preferred.
\end{enumerate}
\section{Predicting acceleration - categorical crossentropy}
\label{chapter5sec:cce}
Now that the basic criteria for training is fixed such as which dataset, activation, and
loss functions, we can move ahead and optimise the predicted classes by tuning the
neutral network.

\subsection*{LSTM vs Non-LSTM}
Before going into tuning the neural network, it is necessary to tell that acceleration
prediction needs temporal information; meaning decision to drive slower or faster depends
on the previous, historical frames. LSTM is used for this purpose. When non-LSTM model is used
to predict acceleration, it only predicts for the current frame(doesn't provide past
frames information) which often results in vehicle being stationary.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{0.9\textwidth}
    \input{figures/inkscape/lstmvsnolstm1.pdf_tex}
    \caption{Datasets 3 - No LSTM vs LSTM comparison}
    \label{fig:ds3nolstmvslstm}
\end{figure}
For our setup, we choose a $timestep = 15$. That means acceleration of current time frame
is predicted using previous 14 time frames.
\subsubsection*{Determining the optimal LSTM output units}
In Keras, the LSTM units refer to the dimension of hidden state vector \textit{h} that is the state output from RNN cell.
The table \ref{table:unitsvstime}, compares different unit values and the number of
trainable parameters(weight that can be trained during backpropagation) at this LSTM
layer.
In our case, it means that for a time series of 15, there will be 15 \textit{cell states},
15 \textit{hidden states}, and 15 \textit{outputs} each of vector size defined by the
units in the table such as 20, 60 or 100.

Upon evaluation with these different units, 100 output units though has the highest
trainable parameters for this layer alone, retains more information needed for training
the model.  Hence, a LSTM unit of 100 is chosen.
\begin{table}[t]
    \centering
\begin{tabular}{ccc}
    \toprule
    LSTM Output Units & Trainable Parameters(ca.) & Processing time needed \\\midrule
    20 & 20000 & 1hr 44m  \\
    60 & 61000 & 1hr 42m \\
    100 & 434000  & 1hr 40m \\\bottomrule
\end{tabular}
\caption{LSTM Output Units vs Trainable Parameters vs Training time}
\label{table:unitsvstime}
\end{table}
%\newpage
\subsection{Basic Model}
\begin{figure}[!ht]
    \centering
    \def\svgwidth{0.2\textwidth}
    \input{figures/inkscape/steeringbasicmodel.pdf_tex} %use full path to know the location of pdftex
    \caption{Basic model}
    \label{fig:steeringbasicmodel}
    %\vspace*{-0.41in}
\end{figure}

For training, a model as shown in fig. \ref{fig:steeringbasicmodel}, is designed and its
result is seen in fig. \ref{fig:ds3categoricalcrossentropybasic}. Interestingly, the
training loss curve \textit{follows} the classification loss as it dominates the model. Steering
loss however, after epoch 32 starts to increase. Sure enough, upon evaluation,
the steering is all over the place and acceleration is not stable at all, resulting in
many collisions as shown in table \ref{table:softmaxandcce} (dataset 3 column).

The cause for this behaviour is investigated and it is found that different losses have
different magnitudes. By forcing the neural network to learn/train both classification and
regression losses from a same dense layer causes instability in learning.
\begin{figure}[t]
	\centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/categoricalcrossds3basic.pdf_tex} %use full path to know the location of pdftex
    \caption{Basic model}
    \label{fig:ds3categoricalcrossentropybasic}
   % \vspace*{-0.30in}
\end{figure}
%\vspace{-.2in}
\subsection{Splitting at the dense layers}
To alleviate some of the burden the second dense layer(dense 2) is split into two separate dense
layers; one for classification outputs and other for steering as show in fig. \ref{fig:steeringdensesplit}. The result \ref{fig:ds3categoricalcrossentropydense}, stops
the strange steering loss increase. Upon evaluation, this model shows better steering
control but still the acceleration is not stable or consistent as shown in table
\ref{table:ccedense}.
\begin{figure}[!ht]
	\centering
    \def\svgwidth{0.2\textwidth}
    \input{figures/inkscape/steeringdensesplit.pdf_tex} %use full path to know the location of pdftex
    \caption{Split at the second dense layer}
    \label{fig:steeringdensesplit}
\end{figure}

\begin{figure}[!h]
	\centering
    \def\svgwidth{0.9\textwidth}
    \input{figures/inkscape/categoricalcrossds3dense1.pdf_tex} %use full path to know the location of pdftex
    \caption{Separate dense layers for classification and steering}
    \label{fig:ds3categoricalcrossentropydense}
\end{figure}

\begin{table}[h]
    \centering
\begin{tabular}{lc}
    \toprule
    Criteria(Softmax/Categorical crossentropy). Rating 1 to 5. 1 being the lowest & Dataset 3 \\\midrule
    Lane keeping/Drive straight  & 4  \\
    Gradual acceleration increase  & 3 \\
    Smooth braking behaviour observed & 3 \\
    Smooth steering control at high speed(10m/s) & 2 \\
    Smooth steering control at turnings & 1\\
    Avoids colliding into static objects & 1 \\
    Detects vehicles as dynamic objects & 5 \\
    Navigates traffic smoothly & 3\\
    Stops at random places & 5 \\\bottomrule
\end{tabular}
\caption{Separate dense layers - How the model evaluates to different criteria}
\label{table:ccedense}
\end{table}
\newpage
\subsection{Splitting at the LSTM layers}
Continuing the theme of tuning the network, the model is split further at LSTM layer as shown in fig. \ref{fig:steeringlstmsplit}. The main aim here is to see if steering control improves and
is stable for considerable acceleration prediction. From fig. \ref{fig:ds3categoricalcrossentropylstm}, the steering loss gets a marginal gain. Still at
evaluation, the trained models stops at random places, and steering control is not stable at
higher acceleration predicted values. Hence the predicted acceleration value is reduced by
50-70\% and fed to the controller. Sure enough the vehicle exhibits stable movements as
seen from table \ref{table:cceLSTM}.

\begin{figure}[!ht]
	\centering
    \def\svgwidth{0.25\textwidth}
    \input{figures/inkscape/steeringlstmsplit.pdf_tex} %use full path to know the location of pdftex
    \caption{Split at the LSTM layer}
    \label{fig:steeringlstmsplit}
\end{figure}

\begin{figure}[!ht]
	\centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/categoricalcrossds3lstm1.pdf_tex} %use full path to know the location of pdftex
    \caption{Separate LSTM layers for classification and steering}
    \label{fig:ds3categoricalcrossentropylstm}
\end{figure}
\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Criteria(Softmax/Categorical crossentropy). Rating 1 to 5. 1 being the lowest & Dataset 3 \\\midrule
    Lane keeping/Drive straight  & 4  \\
    Gradual acceleration increase  & 3\\
    Smooth braking behaviour observed & 3 \\
    Smooth steering control at high speed(10m/s) & 4 \\
    Smooth steering control at turnings & 1\\
    Avoids colliding with static objects & 2 \\
    Detects vehicles as dynamic objects & 5 \\
    Navigates traffic smoothly & 4\\
    Stops at random places & 5 \\
    Smooth evaluation experience & 3 \\\bottomrule

\end{tabular}
\caption{Split at the LSTM layer - How the model evaluates to different criteria}
\label{table:cceLSTM}
\end{table}

\subsection{Using two different NN for acceleration and
Steering}
It can be deduced that optimised weights play an important role on how it influences
steering and acceleration prediction. The model is now split as two different neural
networks(NN) as seen in fig. \ref{fig:steeringnnsplit}. Though the result
\ref{fig:ds3categoricalcrossentropy2nn} looks similar to
\ref{fig:ds3categoricalcrossentropylstm}, the model predicts stable, consistent
acceleration values.

From table \ref{table:cce2NN}, it even exhibits occasional turning behaviours at junctions. When it
is exposed to traffic, the model does well to navigate, brake, and accelerate.
\begin{figure}[!ht]
	\centering
    \def\svgwidth{0.25\textwidth}
    \input{figures/inkscape/steeringNNsplit.pdf_tex} %use full path to know the location of pdftex
    \caption{Separate NN training model}
    \label{fig:steeringnnsplit}
\end{figure}

\begin{figure}[!ht]
	\centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/categoricalcrossds32nn1.pdf_tex} %use full path to know the location of pdftex
    \caption{Separate neural network for Classification and Steering}
    \label{fig:ds3categoricalcrossentropy2nn}
\end{figure}
\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Criteria(Softmax/Categorical crossentropy). Rating 1 to 5. 1 being the lowest & Dataset 3 \\\midrule
    Lane keeping/Drive straight  & 4  \\
    Gradual acceleration increase  & 4\\
    Smooth braking behaviour observed & 4 \\
    Smooth steering control at high speed(10m/s) & 4 \\
    Smooth steering control at turnings & 2\\
    Avoids colliding with static objects & 3 \\
    Detects vehicles as dynamic objects & 5 \\
    Navigates traffic smoothly & 3\\
    Stops at random places & 5 \\
    Smooth evaluation experience & 4 \\\bottomrule
\end{tabular}
\caption{Separate neural network for classification and steering outputs - How the model evaluates to different criteria}
\label{table:cce2NN}
\end{table}

A quick overview of steering losses across different NN changes is shown in fig. \ref{fig:ds3categoricalcrossentropysteeringcompare}.
 \begin{figure}[!ht]
	\centering
    \def\svgwidth{0.9\textwidth}
    \input{figures/inkscape/categoricalcrossds3steeringCompare1.pdf_tex} %use full path to know the location of pdftex
    \caption{Steering command loss comparison}
    \label{fig:ds3categoricalcrossentropysteeringcompare}
\end{figure}

\subsection{Observations}
\begin{enumerate}
    \item Tuning the neural network albeit only the fully connected layers, results in
        optimised prediction of steering control corresponding to the classification
        outputs.
    \item The car exhibits random, unknown stops at random places. The actual reason
        behind it is unknown but it is suspected that since the dataset has imbalanced
        classes, it contributes to this random decisions.
    \item Separating into two NNs allows better steering control at a higher speed.
    \item Sunlight and shadows still play a major role. They do some random, strange
        things to models that eventually lead to crashes out-of-nowhere. It could be
        deduced that some buildings' shadows could be considered as static or dynamic
        objects.
\end{enumerate}
\section{Velocity}
Velocity is a scalar value, labelled output. It is considered as an \textit{auxiliary
task}. An auxiliary task usually consists of estimating quantities that are relevant to
solving main supervised learning problem. That means that velocity is predicted as an
auxiliary task using existing CNN-LSTM-Dense architectures without affecting the major
tasks i.e., predicting acceleration and steering.

The images are fed into the models shown in fig. \ref{fig:velocitycompareNN} and results are
compared as how predicting velocity affects acceleration and steering.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/velocitycompareNN.pdf_tex} %use full path to know the location of pdftex
    \caption{Different architectures used while predicting velocity}
    \label{fig:velocitycompareNN}
\end{figure}

From the fig. \ref{fig:velocitycompareloss1}, looking at the loss-epoch graphs of model
\textit{a}, \textit{b}, \textit{c}, the validation losses follow velocity's validation loss.
When compared to classification and steering loss, this loss is too high. Sure enough
while evaluating steering and acceleration are worse. Since velocity is an auxiliary task,
there is a need to rethink how cost function is calculated.
\begin{figure}[!ht]
	\centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/velocitycomparelosses5.pdf_tex} %use full path to know the location of pdftex
    \caption{Comparison of losses for NN architectures shown in fig. \ref{fig:velocitycompareNN}}
    \label{fig:velocitycompareloss1}
\end{figure}

\subsection{Weighted loss function}
Since velocity is taking control of the neural network to provide feature extraction and
decision making for its prediction, both acceleration and steering suffer bad
consequences. In order to avoid this, weighted cost function is introduced to the three
outputs. As velocity is an auxiliary task, it is suppressed more than others. From figures
\ref{fig:velocitycompareNN} and \ref{fig:velocitycompareloss1}, model \textit{a} gives
nearly optimum losses. So that model is chosen to carry out this weighted cost function
experiment.

The model is tweaked only to include custom, weighted cost function. The trained model
exhibits now losses similar to classification losses than velocity(as seen in fig. \ref{fig:velocitycompareloss1}, \textit{weighted} graph). Indeed during
evaluation, acceleration and steering both get preference and control than velocity.
Hence it is possible to include auxiliary tasks to normal NNs without compromising the
actual functionality of it.
\subsection*{Observation}
\begin{enumerate}
    \item Including an auxiliary task such as velocity needs to be custom weighted to
        allow the primary function of the NN to be fulfilled.
    \item Velocity prediction has a high loss. More analysis needs to be done tweaking the
        NN model differently and running experiments.
\end{enumerate}
\section{Convolution layers manipulation}
The fully connected/dense layers are tweaked to various designs as shown in
figures \ref{fig:steeringbasicmodel}, \ref{fig:steeringdensesplit},
\ref{fig:steeringlstmsplit}, \ref{fig:steeringnnsplit} and having convolutional layers as
constant. Some interesting possibilities and observations are made possible. As a
consequence, the convolutional layers are changed keeping dense layers as constant.
As velocity prediction is also included, \ref{fig:velocitycompareNN}{a} model is
considered.

The convolutional layers can be adjusted either by width(changing the feature maps or
stride parameter) or depth(changing the number of layers).
\subsection{Adjusting the width of the convolutional layers}
\subsubsection*{Changing the feature maps channel depth}
The convolutional layers consists of feature maps channels, kernel filter which convolves
on the input using a specified stride. In our case, the last convolutional layer's feature
map channel's depth is increased from 64 to 80 (as shown in fig. \ref{fig:convlayerschange1}a ). This change, after \textit{flatten} layer,
increases the trainable parameters. This allows for more features to carried into the fully connected layer.
\begin{figure}[!ht]
%	\centering
    \def\svgwidth{1\textwidth}
    \input{figures/inkscape/velocitysplitconvlayerschange6.pdf_tex} %use full path to know the location of pdftex
    \caption{Convolutional layers width changes - Increasing feature maps channel depth}
    \label{fig:convlayerschange1}
\end{figure}
Looking at the loss-epoch graph(fig \ref{fig:convlayerslosses1}) for this experiment, they
give relatively similar results as \ref{fig:velocitycompareloss1}a model.
\subsubsection*{Changing the stride}
Stride is a component in CNN tuned for compressing the images data. This parameter
specifies the movement step of the kernel/filter. In our case, the stride of the 5th
convolutional layer is decreased from (2,2) to (1,1) (see fig.
\ref{fig:convlayerschange1}b ). This ensures the information
tensors are not compressed by half. The flatten layer gets uncompressed, more tensors
which increases the trainable parameters of the features.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/convlayerslosses1.pdf_tex} %use full path to know the location of pdftex
    \caption{Convolutional layers width adjustments - Comparison of losses}
    \label{fig:convlayerslosses1}
\end{figure}
From fig. \ref{fig:convlayerslosses1}, we see that losses are similar to the
\ref{fig:velocitycompareloss1}a model but the classification loss starts to overfit well
ahead of the \textit{a} model. Since the training model follows the classification loss,
the best model is not stored with this change.

\subsection{Adjusting the depth of the Convolutional layers}
\subsubsection*{Increase the convolutional layers to eleven}
Fig. \ref{fig:convlayerschange2}a, shows how the convolutional layers are increased to 11
layers and also the tensor shape before \textit{flatten} layer is made greater than the
previous case(a model). The change in losses because of these adjustments is instant.
Fig. \ref{fig:convlayerslosses2} shows that the model overfits as soon as epoch 10. The
model follows the classification loss. Though other losses continue to decrease, these
events are not recorded.
\begin{figure}[!ht]
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/velocitysplitconvlayerschange5.pdf_tex} %use full path to know the location of pdftex
    \caption{Convolutional layers depth changes - Increasing the number of CNN layers}
    \label{fig:convlayerschange2}
\end{figure}
\subsubsection*{Increase the convolutional layers to eight}
If the convolutional layers are increased only to 8 and the tensor shape is kept as small
as possible before flatten layer, the loss-epoch graph \ref{fig:convlayerslosses2} shows
better behaviour than increasing to 11 layers. However, this change also brings about
overfitting behaviour. It could be seen that the velocity and steering losses converge
well. However, like before, they are not recorded.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/convlayerslosses2.pdf_tex} %use full path to know the location of pdftex
    \caption{Convolutional layers depth adjustments - Comparison of losses}
    \label{fig:convlayerslosses2}
\end{figure}
\section{Depth Camera}
The depth camera is typically used to measure the distance between objects. In our case,
we use it to provide additional spatial information between vehicles and objects. We test
how the neural networks adapts and predicts acceleration, braking and steering.

The fig. \ref{fig:colourvsdepth1} loss-epoch graphs show a little better performance compared to
colour-RGB images trained model. For training the architecture
\ref{fig:velocitycompareNN}a is used.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/colourvsdepth1.pdf_tex} %use full path to know the location of pdftex
    \caption{Comparison of losses between using RGB-Grayscale and depth images}
    \label{fig:colourvsdepth1}
\end{figure}
Upon evaluation, however, as populated in the table \ref{table:depthsensorstandalone},
the results are evident; it is bad. Because the classification loss is bad, the
acceleration doesn't work at all. Steering works but since depth images don't have
features such as lane markings in their images, the car can't right itself when it is on
the boundary.

Surprisingly, the model recognises traffic vehicle when driven at a very slow speed. It
even stops in front of traffic in some situations. Static objects are also recognised and
sometimes the model predicts correct steering angles to avoid collision.

So this standalone training lets us to observe that with depth sensor images, it is
possible to recognise other vehicles and static objects like pavement. In the next
section, we explore data fusion by combining RGB and depth images.
\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Criteria. Rating 1 to 5. 1 being the lowest & Rating \\\midrule
    Lane keeping/Drive straight  & 1  \\
    Gradual acceleration increase  & 1\\
    Smooth braking behaviour observed & 1 \\
    Smooth steering control at high speed(10m/s) & 1 \\
    Smooth steering control at turnings & 1\\
    Avoids colliding with static objects & 3(at slow speed) \\
    Detects vehicles as dynamic objects & 4 \\
    Navigates traffic smoothly & 1\\
    Stops at random places(negative case) & 5 \\
    Smooth evaluation experience & 1 \\\bottomrule
\end{tabular}
\caption{Depth sensor evaluation - standalone}
\label{table:depthsensorstandalone}
\end{table}
\section{Segmentation camera images}
The segmented images consist of instance tags which are initialised in the LGSVL sensor
parameters. They provide colour instance of each object on the map. For eg. car is pink,
pedestrian red, barriers white etc. So a training with just segmented camera images is
done and the losses-epoch graph(fig. \ref{fig:colourvsseg1}) is plotted. The graph shows
that it performs similar to RGB-Grayscale training model.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/colourvseg.pdf_tex} %use full path to know the location of pdftex
    \caption{Comparison of losses between using RGB-Grayscale and Segmented images}
    \label{fig:colourvsseg1}
\end{figure}
\section{Data Fusion}
It is evident that using a different source of image other than RGB, makes the model not
recognise enough features. In RGB image models alone we see the velocity prediction is bad. So we need
something more than RGB images to predict these parameters.

\subsection{Early fusion}
As explained in chapter \ref{chapter:fundamentals}, section \ref{sec:datafusion}, early
fusion involves combining different sources as one and feeding into the network. Here we
take different sources of images such as RGB, depth, and segmented images, fuse two or all
of them together and feed it to the NN as shown in fig. \ref{fig:Datafusiontypes}a.
\subsubsection*{RGB-Grayscale and Depth images}
The table \ref{table:paramsEF} gives the changes in hyperparameters made while conducting
this technique.

\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Setting  & Value  \\\midrule
    Feature maps  & [24, 36, 48, 64, 80]  \\
    Input  & (15, 70, 160, 2)\\\bottomrule
\end{tabular}
\caption{Hyperparameter setting changes - Early fusion}
\label{table:paramsEF}
\end{table}
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/fusionlosses1.pdf_tex}
    \caption{Comparison of fusion losses}
    \label{fig:datafusionlosses1}
\end{figure}
From the graph \ref{fig:datafusionlosses1}, it can be seen that early fusion of
RGB-Grayscale and depth images, performs better than standalone versions of either of
them. The overall loss which follows the classification loss is lower than standalone
variants. However, looking at loss graph alone isn't a good judge of the model.

Sure enough, upon evaluation, table \ref{table:earlyfusionrgbdepth} shows the model
performs slightly better to static objects. However, it fails to acceleration consistently
and braking is observed regularly observed.
\begin{table}[!ht]
    \centering
\begin{tabular}{lcc}
    \toprule
    Criteria. Rating 1 to 5. 1 being the lowest & Rating \\\midrule
    Lane keeping/Drive straight  & 3  \\
    Gradual acceleration increase  & 3\\
    Smooth braking behaviour observed & 1 \\
    Smooth steering control at high speed(10m/s) & 1 \\
    Smooth steering control at turnings & 1\\
    Avoids colliding with static objects & 3(at slow speed) \\
    Detects vehicles as dynamic objects & 4 \\
    Navigates traffic smoothly & 1\\
    Stops at random places & 5 \\
    Smooth evaluation experience & 1 \\\bottomrule
\end{tabular}
\caption{Early fusion evaluation - RGB-Grayscale and Depth images}
\label{table:earlyfusionrgbdepth}
\end{table}


\subsubsection*{RGB-Grayscale and Segmented images}
For these image sources, the same hyperparameter setting used for RGB-G-Depth is also used
here.

From the graph \ref{fig:datafusionlosses1}, this combination of images performs almost
similar to standalone RGB-G. It might be because the segmented has only 11 instances of
objects where they are coloured differently using tags. This image, however, due to
limitation of computing resources, is converted to grayscale, taking away some of its
ability.

After evaluation table \ref{table:earlyfusionrgbseg} is tabulated with results. The
results are positive and the car navigates smoothly when introduced to traffic.
\begin{table}[!ht]
    \centering
\begin{tabular}{lcc}
    \toprule
    Criteria. Rating 1 to 5. 1 being the lowest & Rating \\\midrule
    Lane keeping/Drive straight  & 4  \\
    Gradual acceleration increase  & 4\\
    Smooth braking behaviour observed & 2 \\
    Smooth steering control at high speed(10m/s) & 3 \\
    Smooth steering control at turnings & 4\\
    Avoids colliding with static objects & 3 \\
    Detects vehicles as dynamic objects & 5 \\
    Navigates traffic smoothly & 2\\
    Stops at random places& 5 \\
    Smooth evaluation experience & 4 \\\bottomrule
\end{tabular}
\caption{Early fusion evaluation - RGB-Grayscale and Segmented images}
\label{table:earlyfusionrgbseg}
\end{table}

\subsection{Late fusion}
Similar to early fusion, late fusion explained in section \ref{sec:datafusion}, is attempted
using \ref{fig:Datafusiontypes}b as reference. The images from different sources
are separately fed to the NN and after two convolutional layers,
their outputs are fused. Then the fused output is continued with the remaining layers.
\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Setting  & Value  \\\midrule
    Feature maps  & [24, 36] [24, 36], [80, 96, 96]  \\
    Input  & (15, 70, 160, 2)\\\bottomrule
\end{tabular}
\caption{Hyperparameter setting changes - Late fusion}
\label{table:paramsLF}
\end{table}

\subsubsection*{RGB-Grayscale and Depth images}

The table \ref{table:paramsLF} gives the changes in hyperparameters made while conducting
this technique.

We have seen from loss graph \ref{fig:convlayerslosses2}, increasing the capacity of
convolution layers for these images' dimension, makes the model overfit quicker than
standalone variants. Just so, the graph for this experiment \ref{fig:datafusionlosses1},
overfits earlier and faster than other methods. We, however, see a minuscule gain in
classification loss and good improvement in velocity and steering losses.

To verify these observation, the model is tested and table \ref{table:latefusionrgbdepth}
is filled up. Just like its early fusion counterpart, the prediction is not good as hoped
from the losses graph. It does certain things really well but not on consistent basis.
\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Criteria. Rating 1 to 5. 1 being the lowest & Rating \\\midrule
    Lane keeping/Drive straight  & 4  \\
    Gradual acceleration increase  & 1\\
    Smooth braking behaviour observed & 1 \\
    Smooth steering control at high speed(10m/s) & 1 \\
    Smooth steering control at turnings & 1\\
    Avoids colliding with static objects & 3 \\
    Detects vehicles as dynamic objects & 3 \\
    Navigates traffic smoothly & 1\\
    Stops at random places & 5 \\
    Smooth evaluation experience & 1 \\\bottomrule
\end{tabular}
\caption{Late fusion evaluation - RGB-Grayscale and Depth images}
\label{table:latefusionrgbdepth}
\end{table}

\subsubsection*{RGB-Grayscale and Segmented images}
The losses \ref{fig:datafusionlosses1} for this experiments are nearly identical to its early fusion variants.
Indeed while evaluating, the model performs great to tough situations such as avoiding
collision to barriers or pavements. Of course, there are a few problems which need further
analysis to resolve.
\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Criteria. Rating 1 to 5. 1 being the lowest & Rating \\\midrule
    Lane keeping/Drive straight  & 5  \\
    Gradual acceleration increase  & 2\\
    Smooth braking behaviour observed & 3 \\
    Smooth steering control at high speed(10m/s) & 3 \\
    Smooth steering control at turnings & 4\\
    Avoids colliding with static objects & 4 \\
    Detects vehicles as dynamic objects & 5 \\
    Navigates traffic smoothly & 3\\
    Stops at random places & 5 \\
    Smooth evaluation experience & 4 \\\bottomrule
\end{tabular}
\caption{Late fusion evaluation - RGB-Grayscale and Segmented images}
\label{table:latefusionrgbseg}
\end{table}
\subsection*{Observations}
\begin{enumerate}
    \item Fusing different sources give new dimension to the model which helps in making
        more correct decisions.
    \item Fusing RGB-G and depth images don't seem to match the expectations.
    \item Fusing RGB-G and segmented images produce some great results. It would be a
        good avenue to pursue future analysis.
\end{enumerate}
\section{Extending the RGB-G+Segmented early fusion to larger dataset}
With the promise shown by RGB-G and segmented images fusion evaluation, it is necessary to
try to increase the dataset with more robust data, train the model, and test it. Hence the
dataset 3 is increased from 100,000 to 224,000 data entries consisting also change in
weather conditions and night time driving. With this dataset, the same setup as RGB-G and
segmented image, early fusion is used.
\begin{figure}[!ht]
    \centering
    \def\svgwidth{\textwidth}
    \input{figures/inkscape/fusionlosses224k.pdf_tex}
    \caption{Comparison of losses between Segmented images vs 100k RGB-G+Seg vs 224k
    RGB-G+Seg models. \textit{Lr} denoted learning rate.}
    \label{fig:224kdatafusionlosses2}
\end{figure}
From the losses-epoch graph fig. \ref{fig:224kdatafusionlosses2}, the classification
losses can be prevented from overfitting by increasing the data and manipulating the
learning rates. For this training alone, learning rate is started with $10^{-4}$ and
continued to decrease every 20 epochs. Since the learning rate is decreased, the training
epochs is increased to 150-200. However, for the sake of comparison only 50 epochs is
shown.

The table \ref{table:earlyfusionrgbseg200k} shows training with more data helps in
performance. The model is unaffected by sunlight, weather changes or time of the day. It
performs quite good even in night time conditions. This is possible only because of
segmentation sensor camera images as it clearly defines the lane boundaries, cars etc.,
Though, there is less-to-no control at high speeds, this training model performs the best
among all other trainings.
\begin{table}[!ht]
    \centering
\begin{tabular}{lc}
    \toprule
    Criteria. Rating 1 to 5. 1 being the lowest & Rating \\\midrule
    Lane keeping/Drive straight  & 5  \\
    Gradual acceleration increase  & 3\\
    Smooth braking behaviour observed & 2 \\
    Smooth steering control at high speed(10m/s) & 2\\
    Smooth steering control at turnings & \textbf{5}\\
    Avoids colliding with static objects & \textbf{5} \\
    Detects vehicles as dynamic objects & 4\\
    Navigates traffic smoothly & 2\\
    Affected by sunlight & \textbf{4} \\
    Affected by low-light or night driving & \textbf{4} \\
    Stops at random places & 5 \\
    Smooth evaluation experience & 5 \\\bottomrule
\end{tabular}
\caption{Larger dataset early fusion evaluation - RGB-Grayscale and Segmented images}
\label{table:earlyfusionrgbseg200k}
\end{table}
\begin{figure}[!ht]
    \centering
    \def\svgwidth{0.5\textwidth}
    \input{figures/inkscape/nightdriving.pdf_tex}
    \caption{Night time driving with RGB-G and Segmented images fusion - Traffic vs
    collisions}
    \label{fig:224kdatafusionnight}
\end{figure}
\subsection*{Observations}
\begin{enumerate}
    \item Adding more data to the training helps in reducing the classification loss
        further resulting in a little better prediction of acceleration.
    \item With the help of new labelled data exposed different weather conditions, the
        trained model now performs good at wet conditions.
    \item Night time driving which was impossible(see fig. \ref{fig:dsvslcvstrafficAll}a),
        is now made possible with fusing segmentation sensor images. The chart
        \ref{fig:224kdatafusionnight} shows good improvement in number of collisions
        while driving at night.
\end{enumerate}
